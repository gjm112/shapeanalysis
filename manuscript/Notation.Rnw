\documentclass{article}
% This is the recommended preamble for your document.
\usepackage{setspace}
\usepackage{url}
%% Load De Gruyter specific settings 
\usepackage{dgjournal}          
\usepackage{color,soul}
%% The mathptmx package is recommended for Times compatible math symbols.
%% Use mtpro2 or mathtime instead of mathptmx if you have the commercially
%% available MathTime fonts.
%% Other options are txfonts (free) or belleek (free) or TM-Math (commercial)
\usepackage{amssymb,amsmath}
\usepackage{multirow}
%% Use the graphics package to include figures
\usepackage{graphics}
%% Use natbib with these recommended options
\usepackage[authoryear,comma,longnamesfirst,sectionbib]{natbib} 
\usepackage{lscape}
\newcommand{\hlc}[2][yellow]{{\sethlcolor{#1}\hl{#2}}}
\setstretch{1}
\setlength{\bibsep}{10pt}
\usepackage{booktabs}

\usepackage{subfigure}% Support for small, `sub' figures and tables. Your choice of alternative may be preferred instead.
\usepackage{color,soul}
\usepackage{moreverb}
\usepackage{graphicx}
\usepackage[table]{xcolor}









\begin{document}
\section{Introduction}
KAJAL's (and grady) Lit review


\section{Notation}
Consider a closed curve: 
$$
C: \mathbb{S} \rightarrow \mathbb{R}^2 
$$

Shapes should be invariant across several transformations including translation, scaling, rotation, and reparameterizaion.  Specifically, the shape $C$ is defined to be the equivalence class

$$
C = \{\sigma O C(\gamma(t)) + a, \gamma \in \Gamma, O \in SO(2), a\in \mathcal{R}^2, \sigma > 0\} 
$$

%Consider the curve $C_j^m: \mathbb{S} \rightarrow \mathbb{R}^2, j = 1, \dots, n_m$ which is only observe on some region (don't want to use R here...) $S_j \subset \mathbb{S}$.  The goal here is to impute the missing part of each shape.  Then perform some type of complete shape analysis.  




\section{Notation}
% $$
% C: [0,1] \rightarrow \mathbb{R}^2 \times \left\{0,1\right\}^2
% $$
Consider a closed curve, $C$, in two dimensions: 
$$
C: \mathbb{S} \rightarrow \mathbb{R}^2 
$$.

Since, shapes should be invariant across several transformations including translation, scaling, rotation, and reparameterizaion, the shape $C$ is defined to be the equivalence class

$$
C = \{\sigma O C(\gamma(t)) + a, \gamma \in \Gamma, O \in SO(2), a\in \mathcal{R}^2, \sigma > 0\} 
$$. 

Consider a set of shapes $\mathcal{C}$, containing some fully observed shapes and some partially observed shapes.  We define the set of fully observed shapes to be called 
$$
\mathcal{C}^o = (C^o_1 \cdots C^o_{n_o})
$$

whereas the set of partially observed shapes is the referred to as 

$$
\mathcal{C}^m = (C^m_1 \cdots C^m_{n_m})
$$.  

Each $C^m_j$ consists of a the observed part $C^m_{j,obs}$ and the missing part $C^m_{j,mis}$ such that $C^m_j = (C^m_{j,obs},C^m_{j,mis})$.  That is 

$$
C^m_{j,mis} = C_j: [a,b] \rightarrow \mathbb{R}^2
$$
where
$$
[a,b] = {x \in \mathcal{S} | R'_j(x) = 1}
$$.

So 

$$
\mathcal{C}^m_{obs} = (C^m_{1,obs} \cdots C^m_{n_m,obs})
$$

and 

$$
\mathcal{C}^m_{mis} = (C^m_{1,mis} \cdots C^m_{n_m,mis})
$$

We then have 

$$
\mathcal{C} = (\mathcal{C}^o,\mathcal{C}^m) = (\mathcal{C}^o,\mathcal{C}^m_{mis},\mathcal{C}^m_{obs})
$$ 

and $n = n_o + n_m$. \\

In traditional missing data settings, a missingness indicator is defined such that it is 1 when a data point is missing and 0 otherwise.  In this setting we define a function indicating which part of the function is observed and which is missing.  This function is called $R$ and is defined as follows: 

$$
R: \mathbb{S} \rightarrow \left\{0,1\right\}^2
$$

While this function allows for a curve to be missing the $x$ or $y$ value individually at a given point in $\mathbb{S}$, in our setting both the $x$ and $y$ values are either both observed or both missing, and the indicator function in our setting is defined as follows: 

$$
R': \mathbb{S} \rightarrow \left\{\{0,0\},\{1,1\}\right\}
$$

We will work with $R'$ as a missingness indicator function here, and it is 1 when the values of $(x(t),y(t))$ are both unobserved and 0 if both $(x(t),y(t))$ are observed where $t \in \mathbb{S}$.  The set of all missingness functions is defined to be $\mathcal{R} = (R'_1, \cdots, R'_n)$.     \\ 

Next, we want to consider the joint distribution of the set of curves $\mathcal{C}$ and the set of missingness indicator functions $\mathcal{R}$.  

%How do I say this: However, since the shape spaces that we are interested in this context are non-linear and infinite dimensional we need to take care when defining a probability density function over this space.  Specifically we are interested in defining this density function over a manifold $M = \mathbb{S}$ as opposed to the manifold that is used in traditional statistical settings: $M = \mathbb{R}^n$.  

Next we want to consider the joint distribution of the curves with the missingnes indicator.  

$$
P(\mathcal{C}, \mathcal{R}) = P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R})
$$

In order to perform imputations we first need to define a model for the likelihood: 

$$
P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R}| \theta, \phi)
$$

where $\theta$ is a vector of parameters associated with modeling the data (i.e. $\mathcal{C}$) and $\phi$ are parameters associated with the model of the missing data mechanism (i.e. $\mathcal{R}'$). 

We can then place a prior distribution on the parameters $p(\theta, \phi)$ which will lead to a posterior probability: 

$$
P( \theta, \phi|\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R})\propto P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R}| \theta, \phi) \times p(\theta, \phi)
$$


We then seek this distribution: 
$$
\int\int P(C_{mis}|\theta, \phi) P(\theta, \phi|C_{obs}, C_{mis}, R ) d\theta d\phi = P(C_{mis}|C_{obs},R)
$$

Random draws from the distribution $P(C_{mis}|C_{obs},R)$ can then be used to to fill in the missing part of the shape. 


Each partially observed shape can be completed $M$ times.  This yields $M$ sets of completed shapes.  

\section{Combining}
\subsection{Shape Analysis}
Let's say we wanted to ask a statistical question where the answer was a shape (e.g What is the average shape?).  I propose to answer this question, you first find the karcher mean within each of the $M$ completed data sets and then you find the karcher mean of the $M$ means across the completed data sets. This is analagous to Rubin's conbining rules. \\

It would be nice to try to come up with something like a "95 percent shape shadow".  Like a band around the tooth showing the uncertainty around the mean shape. \\

\subsection{Traditional Analysis}
There are other types of analysis, like classification, where the results might be probabilities or other traditional statistical quantitities.  I believe that these can be combined across imputatations using Rubin's classic combining rules.  

\section{Theory}

\section{Approximating $P(C_{mis}|C_{obs},R)$}

The distribution $P(C_{mis}|C_{obs},R)$ can be approximated non-parametrically by employing a hot deck type procedure similar to the idea of predictive mean matching in traditional missing data imputation. \\

We first perform a matching step.
Consider a shape $C_{i}^m \in \mathcal{C}^m$ and a 

\section{Simulations Study}

\section{Results}


\section{Conclusions}













What if we took all the complete shapes and aligned them 



We can represent shapes using finite representations.\\
An issue here though is that these are really just one realization from an equivalence class.   \\
MAR here means that the proability of missingness is a function of only the observes part of the shape. This seems inrealistic in our setting.  

\citet{deRuiter2008}
\bibliographystyle{chicago}
\bibliography{shapebib}


\end{document}