\documentclass{article}
% This is the recommended preamble for your document.
\usepackage{setspace}
\usepackage{url}
%% Load De Gruyter specific settings 
\usepackage{dgjournal}          
\usepackage{color,soul}
%% The mathptmx package is recommended for Times compatible math symbols.
%% Use mtpro2 or mathtime instead of mathptmx if you have the commercially
%% available MathTime fonts.
%% Other options are txfonts (free) or belleek (free) or TM-Math (commercial)
\usepackage{amssymb,amsmath}
\usepackage{multirow}
%% Use the graphics package to include figures
\usepackage{graphics}
%% Use natbib with these recommended options
\usepackage[authoryear,comma,longnamesfirst,sectionbib]{natbib} 
\usepackage{lscape}
\newcommand{\hlc}[2][yellow]{{\sethlcolor{#1}\hl{#2}}}
\setstretch{1}
\setlength{\bibsep}{10pt}
\usepackage{booktabs}

\usepackage{subfigure}% Support for small, `sub' figures and tables. Your choice of alternative may be preferred instead.
\usepackage{color,soul}
\usepackage{moreverb}
\usepackage{graphicx}
\usepackage[table]{xcolor}


\begin{document}

\section{Introduction}
Statistical analysis of shape plays an important role in many areas of science such as biology (\cite{OHiggins1989}, \cite{OHigginsDryden1993}, \cite{GoodallLange1989}), chemistry (\cite{DrydenEtAl2007,CzogielEtAl2011}), medicine (\cite{Bookstein1996, BrignellEtAl2010}), bioinformatics (\cite{GreenAndMardia2006}), genetics (\cite{HorganEtAl1992}), geology (\cite{Lohman1983}) and anthropology \cite{Brophy2014}(Matthews 2017).  Formal techniques for statistical shape analysis have been developed to extend classical statistical methods to shapes, such as computation of a mean shape and shape variability, extensions of Hotelling's $T^2$-test for inference about mean shapes, and principal components analysis of shapes (PCA), to name just a few \cite{DrydenAndMardiaBook}.  Statistical shape analysis techniques generally assume that the entire shape is observed.  However, in practice, there are instances where the shapes of interest are only partially observed.  For example, the shape of the occlusal surface of isolated fossil teeth from the Family Bovidae is useful to biological anthropologists for identifying the taxa of specimens, which in turn is used to reconstruct paleoenvirnoments.  In order to classify these isolated fossil tooth remains, scientists rely on, among other factors, the shape of the outline of the occlusal surface of the tooth to make accurate taxonomic classifications. Currently, only complete (or nearly complete) teeth are generally considered in classification analyses, however, there are numerous specimens of fractured fossil Bovid teeth that are not considered as strongly or ignored entirely in the analysis process because the full shape is not observed.  There are countless examples in the missing data literature where dropping observations with missing data (i.e. complete case analysis) has been demonstrated to produce biased results, therefore, it is scientifically beneficial to have methods for including partial observations in an analysis of interest, in this example the classification of fossilized teeth from the family Bovidae. 

\subsection{Statistical Shape Analysis}
Much of the early work in shape analysis was based on defining shapes by a series of landmarks (\cite{Bookstein1986,MardiaAndDryden1989,Kendall1984,LeAndKendall1993}) where each point represents a meaningful location on a shape and there is a natural correspondence between points across shapes in a sample.  This allows for a natural correspondence of points across shapes and greatly simplifies analysis in many cases, such as computing a mean shape or variability of a collection of shapes.  The use of landmark data also allows for a natural methods for the alignment of shapes to one another using a method called Procrustes analysis \cite{Green1952,Goodall1991}, which aligns shapes subject to translations, rotations, and scaling.  Additionally, shapes can be defined by unlabeled points where there is some true labeling, but it is unobserved.  This setting is more complicated than labeled landmarks since the correspondence between points across shapes is unknown.  In many situations, however, there are no reasonable choices for landmarks on the shape.

Later, \citet{Bookstein1997} defined the concept of slidibng landmarks.   WHAT ARE THESE......explain.  



%Statistical shape analysis is interested in studying a wide variety of questions about shapes from statistical view such as, for example, how different are two shapes, what is the mean / variability of a collection of shapes, and, of particular interest in this manuscipt, shape classification.  In order to work with shapes statistically, a framework needs to be defined for representing shapes, and there are many different idea for doing this.  For instance, shapes can be represented by landmarks \citep{Kendall1984,LeAndKendall1993,MardiaAndDryden1989}. A landmark is a point on a shape that corresponds to a point on another shape across and within populations.  

Sliding landmarks by Bookstein.  

Section 1.2.4 in FSU disserttion is a good description of this.  

Shape analysis in general. 




Partial Shape matching. 
http://diginole.lib.fsu.edu/islandora/object/fsu%3A183299


\section{2.1 - Landmarks}
{\bf A landmark if a point of correspondence on each object that matches between and within populations}

\begin{itemize}
\item scientific landmark (also called anatomical landmarks): Assigned by an expert and meaningful (i.e. corner of the eye)
\item Mathmetical landmarks: Locations for some mathematically relevant reason.  (i.e. high curvature points or extreme points)
\item pseudo-landmarks: Exampel evenly spaced points between two landmarks.  
\end{itemize}

\section{Chapter 2}
ratios of distances between landmarks: "Multivariate morphometrics" Rao 1948
Reyment et al 1984, p120 
Pearson 1926 \

Geometric morhometrics or geometric shape analysis uses the actual points or coordinates.

<<>>=
# library(shapes)
# data(apes)
# size<-centroid.size(apes$x)
# plot(apes$group,size)
@

\section{\texttt{shapes} package in R}  
  
\section{Missing Data}
EM algorithm (Depster Laird and Rubin.  )
Multiple Imputation (Rubin and Little)

\subsection{13.1 - Incomplete Data}
Papers mentioned: 
\begin{itemize}
\item Albers and Gower (2010) - Missing data in procrustes
\item Mitchelson (2013) - handling occluded landmarks. 
\item Gunz et. al. (2009) - missing data in studies of biological evolution of bone surfaces

\end{itemize}




KAJAL's (and grady) Lit review
%\citet{AdamsConroy2005}

%explain closed curves. 
%Then shapes.  
%Then notation for missing data in shapes. 
%which means I now need to talk about open curves.
\section{Notation}
\subsection{Size and Shape}
Consider a closed curve, $C$, 
$$
C: \mathbb{S} \rightarrow \mathbb{R}^2 
$$, 
where $\mathbb{S}$ is the unit sphere.  



%Brian Notation
A shape can then be defined as an equivalence class containing all possible rotations, translations, scalings, and reparameterizations of a closed curve $C$.  Specifically,  a shape $[\beta]$ is formally defined as 
$$
[\beta] = \{\sigma O C(\gamma(t)) + a, \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2, \sigma \in \mathbb{R}^{+} \} 
$$, where $\Gamma$ is the set of all possible warping functions and $SO(2)$ is the special orthogonal group of dimension 2. 

In this particular setting, the preservation of size is of interest and we can define an equivalence class defining the size and shape by removing scaling.  Specifically, size and shape is the equivalence class: 

$$
[\beta^{\star}] = \{ O C(\gamma(t)) + a, \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 \} 
$$



\subsection{Missingness Function}
Now consider a function, $R'$, defined over the set $\mathbb{S}$, which returns a 1 if the shape is observed at that location and a 0 otherwise.  Specifically,   
$$
R': \mathbb{S} \rightarrow \left\{0,1\right\}^2
$$

While this function allows for a curve to be missing the $x$ or $y$ value individually at a given point in $\mathbb{S}$, in our setting both the $x$ and $y$ values are either both observed or both missing, and the indicator function in our setting is defined as follows: 
$$
R: \mathbb{S} \rightarrow \left\{\{0,0\},\{1,1\}\right\}
$$

Now for a specific curve $C$ in the equivalence class $[\beta^{\star}]$, we can define the function $R$.  However, for every reparameterization of $C$ in the equivalence class $[\beta^{\star}]$, there is a corresponding reparameterization of $R$  Also, note that $R^{-1}(1)$ is the subset of domain $\mathbb{S}$ such that the coordinates of the shape are observed for every point in this set.  Further, here it is required that $R^{-1}(1)$ be a connected subset of $\mathbb{S}$.  This prevents us from considering shapes with multiple ``gaps". 

For a given shape, there are many possible reparameterizations of the missingness function, and an equivalence class can be defined for the missingness function across reparametertizations as follows:  

%I think I only need reparameterizations for R?.  
$$
[\rho] = \{ R(\gamma(t)) , \gamma \in \Gamma \} 
$$

Next, we define an equivalence class by combining both size-shape and missingness functions as follows:  
$$
[\beta^{\star}, \rho] = \{ O C(\gamma(t)) + a,  R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 \}
$$ \


Given this we can break up 
$$
[\beta^{\star}_{obs}, \rho] = \{ O C(\gamma(t)) + a,  R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 |  R(\gamma(t)) = 1  \}
$$

%Could also say where $t \in \gamma^{-1}(R^{-1}(1))$

$$
[\beta^{\star}_{mis}, \rho] = \{ O C(\gamma(t)) + a,  R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 |  R(\gamma(t)) = 0  \}
$$

We somehow want to get something like this:  
$$
P(\beta^{\star}_{mis} | \beta^{\star}_{obs}, R)
$$


 


Now this allows us the ability to define the (joint?) equivalence class. 
%Do I need the concept of "joint equivalence class"?  
$$
[\beta^{\star}] = \{ OC(\gamma(t)) + a, R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 )\} 
$$

$$
[\beta^{\star}_{obs}] = \{ OC(\gamma(t)) + a, R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 ) | t \in R^{-1}(1) \subset \mathbb{S}\} 
$$

$$
[\beta^{\star}_{mis}] = \{ OC(\gamma(t)) + a, R(\gamma(t)), \gamma \in \Gamma, O \in SO(2), a\in \mathbb{R}^2 ) | t \notin R^{-1}(1) \subset \mathbb{S}\} 
$$

%I think $R$ needs to be able to handle re-parameterizations but none of the other transformations.  
$$
[\beta^{\star}, \varrho] = ([\beta^{\star}, \varrho]_{obs} \cup [\beta^{\star}, \varrho]_{mis})
$$



\section{Imputation}
Assuming ignorability of the missingness mechanism: 
Draw imputations from: $P([\beta^{\star}_{mis}] | [\beta^{\star}_{obs}])$.


How do we approximate this: $P([\beta^{\star}_{mis}] | [\beta^{\star}_{obs}])$

Let $[\beta^{\star}]_i$ be the $i$-th observation for $i = 1 \cdots n$ where $n$ is the number of fully observed and an observation is the equivalence class containg shape and size information (no scaling allowed).  Additionally, let $[\beta^{\star}_{obs}]_j$ and $[\beta^{\star}_{obs}]_j$ be the observed and missing parts of the $j$-th observation where $j = 1 \cdots n_m$.  



In order to empirically approximate $P([\beta^{\star}_{mis}] | [\beta^{\star}_{obs}])$, we must first compute the distance between $dist([\beta^{\star}_{obs}]_j, [\beta^{\star}]_i)$ for all $i$.  

%How do I say "shape without scaling" sucinctly?  Can I say rigis shape?  Meanng that it doesn't scale? 


%Algorithm Steps
Given a partial shape of fixed size (i.e one open curve) $[\beta^{\star}_{obs}]_j$ and a collection of complete shapes (i.e. closed curves) $[\beta^{\star}]_i$, $i = 1, \cdots, n$, we complete the partial shape using the following algorithm: 
\begin{itemize}
\item Calculate the distance between the partial observed shape of fixed size and each closed curve. That is compute $d_{ij}$ = 
dist($[\beta^{\star}_{obs}]_j$, $[\beta^{\star}]_i$) across all $i$.  
\item C
\end{itemize}





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%Old work
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% $u$ is a connected subset of $\mathbb{S}$
% 
% 
% Partial shape observed over the set $u$.  
% $[\beta^{\star}|_{u}] = \{\beta\circ\gamma | \gamma \in C(\mathbb{S},\mathbb{S}) \}$
% 
% 
% 
% 
% 
% Consider a closed curve, $C$, in two dimensions: 
% $$
% C: \mathbb{S} \rightarrow \mathbb{R}^2 
% $$.
% 
% Since, shapes should be invariant across several transformations including translation, scaling, rotation, and reparameterizaion, the shape $C$ is defined to be the equivalence class
% 
% $$
% C = \{\sigma O C(\gamma(t)) + a, \gamma \in \Gamma, O \in SO(2), a\in \mathcal{R}^2, \sigma > 0\} 
% $$. 
% 
% Consider a set of shapes $\mathcal{C}$, containing some fully observed shapes and some partially observed shapes.  We define the set of fully observed shapes to be called 
% $$
% \mathcal{C}^o = (C^o_1 \cdots C^o_{n_o})
% $$
% 
% whereas the set of partially observed shapes is the referred to as 
% 
% $$
% \mathcal{C}^m = (C^m_1 \cdots C^m_{n_m})
% $$.  
% 
% Each $C^m_j$ consists of a the observed part $C^m_{j,obs}$ and the missing part $C^m_{j,mis}$ such that $C^m_j = (C^m_{j,obs},C^m_{j,mis})$.  That is 
% 
% $$
% C^m_{j,mis} = C_j: [a,b] \rightarrow \mathbb{R}^2
% $$
% where
% $$
% [a,b] = {x \in \mathcal{S} | R'_j(x) = 1}
% $$.
% 
% So 
% 
% $$
% \mathcal{C}^m_{obs} = (C^m_{1,obs} \cdots C^m_{n_m,obs})
% $$
% 
% and 
% 
% $$
% \mathcal{C}^m_{mis} = (C^m_{1,mis} \cdots C^m_{n_m,mis})
% $$
% 
% We then have 
% 
% $$
% \mathcal{C} = (\mathcal{C}^o,\mathcal{C}^m) = (\mathcal{C}^o,\mathcal{C}^m_{mis},\mathcal{C}^m_{obs})
% $$ 
% 
% and $n = n_o + n_m$. \\
% 
% In traditional missing data settings, a missingness indicator is defined such that it is 1 when a data point is missing and 0 otherwise.  In this setting we define a function indicating which part of the function is observed and which is missing.  This function is called $R$ and is defined as follows: 
% 
% $$
% R: \mathbb{S} \rightarrow \left\{0,1\right\}^2
% $$
% 
% While this function allows for a curve to be missing the $x$ or $y$ value individually at a given point in $\mathbb{S}$, in our setting both the $x$ and $y$ values are either both observed or both missing, and the indicator function in our setting is defined as follows: 
% 
% $$
% R': \mathbb{S} \rightarrow \left\{\{0,0\},\{1,1\}\right\}
% $$
% 
% We will work with $R'$ as a missingness indicator function here, and it is 1 when the values of $(x(t),y(t))$ are both unobserved and 0 if both $(x(t),y(t))$ are observed where $t \in \mathbb{S}$.  The set of all missingness functions is defined to be $\mathcal{R} = (R'_1, \cdots, R'_n)$.     \\ 
% 
% Next, we want to consider the joint distribution of the set of curves $\mathcal{C}$ and the set of missingness indicator functions $\mathcal{R}$.  
% 
% %How do I say this: However, since the shape spaces that we are interested in this context are non-linear and infinite dimensional we need to take care when defining a probability density function over this space.  Specifically we are interested in defining this density function over a manifold $M = \mathbb{S}$ as opposed to the manifold that is used in traditional statistical settings: $M = \mathbb{R}^n$.  
% 
% Next we want to consider the joint distribution of the curves with the missingnes indicator.  
% 
% $$
% P(\mathcal{C}, \mathcal{R}) = P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R})
% $$
% 
% In order to perform imputations we first need to define a model for the likelihood: 
% 
% $$
% P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R}| \theta, \phi)
% $$
% 
% where $\theta$ is a vector of parameters associated with modeling the data (i.e. $\mathcal{C}$) and $\phi$ are parameters associated with the model of the missing data mechanism (i.e. $\mathcal{R}'$). 
% 
% We can then place a prior distribution on the parameters $p(\theta, \phi)$ which will lead to a posterior probability: 
% 
% $$
% P( \theta, \phi|\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R})\propto P(\mathcal{C}^o, \mathcal{C}^m_{mis}, \mathcal{C}^m_{obs}, \mathcal{R}| \theta, \phi) \times p(\theta, \phi)
% $$
% 
% 
% We then seek this distribution: 
% $$
% \int\int P(C_{mis}|\theta, \phi) P(\theta, \phi|C_{obs}, C_{mis}, R ) d\theta d\phi = P(C_{mis}|C_{obs},R)
% $$
% 
% Random draws from the distribution $P(C_{mis}|C_{obs},R)$ can then be used to to fill in the missing part of the shape. 
% 
% 
% Each partially observed shape can be completed $M$ times.  This yields $M$ sets of completed shapes.  

\section{Combining}
\subsection{Shape Analysis}
Let's say we wanted to ask a statistical question where the answer was a shape (e.g What is the average shape?).  I propose to answer this question, you first find the karcher mean within each of the $M$ completed data sets and then you find the karcher mean of the $M$ means across the completed data sets. This is analagous to Rubin's conbining rules. \\

It would be nice to try to come up with something like a "95 percent shape shadow".  Like a band around the tooth showing the uncertainty around the mean shape. \\

\subsection{Traditional Analysis}
There are other types of analysis, like classification, where the results might be probabilities or other traditional statistical quantitities.  I believe that these can be combined across imputatations using Rubin's classic combining rules.  

\section{Theory}

\section{Approximating $P(C_{mis}|C_{obs},R)$}

The distribution $P(C_{mis}|C_{obs},R)$ can be approximated non-parametrically by employing a hot deck type procedure similar to the idea of predictive mean matching in traditional missing data imputation. \\

We first perform a matching step.
Consider a shape $C_{i}^m \in \mathcal{C}^m$ and a 

\section{Methods}

\begin{figure}
<<echo = FALSE, message = FALSE>>=
setwd("/Users/gregorymatthews/Dropbox/shapeanalysisgit/")
source("./R/utility.R")
source("./R/curve_functions.R")
source("./R/calc_shape_dist_partial.R")
source("./R/complete_partial_shape.R")
source("./R/impute_partial_shape.R")
source("./R/tooth_cutter.R")

load("./data/data_set_of_full_teeth.RData")
load("./data/ptsTrainList.RData")

tooth <- "LM1"
side <- 1
partial_shape <- t(tooth_cutter(ptsTrainList[[tooth]][[1]])[[side]])
complete_shape <- t(ptsTrainList[[1]][[2]])

scale <- FALSE
plot <- TRUE
 #complete_partial_shape <- function(complete_shape, partial_shape, plot = FALSE, scale = FALSE){
  
  #Dimension
  d <- dim(complete_shape)[1]
  #Number of points for complete_shape and partial_shape
  N_complete <- dim(complete_shape)[2]
  N_partial <- dim(partial_shape)[2]
  
  t <- seq(0,1,length = 100)
  x0 <- matrix(NA,ncol = 100,nrow = 2)
  for (j in 1:d){
    x0[j,] <- (1-t)*partial_shape[j,N_partial] + t*partial_shape[j,1]
  }
  
  partial_shape_closed <- cbind(partial_shape,x0[,2:100])
  
  N_complete_new <- 500
  t <- seq(0,1,length = N_complete_new)
  
  olddel <- get_cumdel(partial_shape_closed)
  
  N <- 100
  
  
  partial_shape_closed_obs <- resamplecurve(partial_shape_closed[,1:(dim(partial_shape_closed)[2] - (dim(x0)[2] - 1))],N)
  partial_shape_closed_mis <- resamplecurve(partial_shape_closed[,(dim(partial_shape_closed)[2] - (dim(x0)[2] - 1)):dim(partial_shape_closed)[2]],N)
  
  #Find the centroid of the observed part
  cent1 <- apply(partial_shape_closed_obs,1, mean)
  
  #Centering
  partial_shape_closed_obs <- partial_shape_closed_obs - cent1
  partial_shape_closed_mis <- partial_shape_closed_mis - cent1
  
  
  if (scale == TRUE){
  #scale factor
  sc1 <- norm(partial_shape_closed_obs, type = "F")
  
  #Scaling the shape
  partial_shape_closed_obs <- partial_shape_closed_obs/sc1
  partial_shape_closed_mis <- partial_shape_closed_mis/sc1
  }
  
  # plot(t(partial_shape_closed_obs))
  # points(t(partial_shape_closed_mis), col = "red")
  # 
  # plot(t(complete_shape))
  
  
  minE <- Inf
  jbest <- NA
  #I think we are looking across all strting points around the curve?
  for (j in 0:(N_complete-1)){
    #What does shiftF do??
    #Why N_complete - 1 and not just N_complete??????
    mu <- ShiftF(complete_shape[,1:(N_complete-1)],j) 
    mu <- cbind(mu,mu[,1])
    
    olddel1 <- get_cumdel(mu)
    
    N <- 100
    library(fdasrvf)
    mu <- resamplecurve(mu,N_complete_new)
    
    newpt1 <- which(t < olddel1[N_partial])
    newpt1 <- newpt1[length(newpt1)]
    
    mu1 <- resamplecurve(mu[,1:newpt1],N) 
    mu2 <- resamplecurve(mu[,newpt1:dim(mu)[2]],N) 
    
    cent2 <- apply(mu1,1,mean)
    mu1 <- mu1 - cent2
    mu2 <- mu2 - cent2
    
    if (scale == TRUE){
    sc2=norm(mu1, type = "F")
    mu1=mu1/sc2
    mu2=mu2/sc2
    }
    
    #Finding the best rotation
    out <- find_best_rotation(partial_shape_closed_obs,mu1)
    R <- out$R
    q2new <- out$q2new
    
    mu1n <- R%*%mu1
    
    Ec <- InnerProd_Q(partial_shape_closed_obs-mu1n,partial_shape_closed_obs-mu1n)
    if (Ec < minE){
      jbest <- j
      Rbest <- R
      complete_shape_obs <- Rbest%*%mu1
      complete_shape_mis <- Rbest%*%mu2
      minE <- Ec
    }
    
  }
  
  
donor <- cbind(complete_shape_obs,complete_shape_mis[,2:dim(complete_shape_mis)[2]])
  
  partial_shape_closed_new <- cbind(partial_shape_closed_obs,partial_shape_closed_mis[,2:dim(partial_shape_closed_mis)[2]])
  
  
  
  
  n <- 40
  tn <- seq(0,1, length = N)
  
  #Why minus 1 in cos?  
  #Defining basic functions
  b <- matrix(NA, nrow = 2*n, ncol = length(tn))
  for (j in 1:n){
    b[j,] <- sin(2*pi*j*tn)/(sqrt(2)*pi*j)
    b[j+n,] <- (cos(2*pi*j*tn)-1)/(sqrt(2)*pi*j)
  }
  
  
  
  #Plot
  par(mfcol = c(5,2))
  par(mar = c(0,0,0,0))
  iter <- 0
  plot(t(donor), type = "l", xlim = c(-100,550), xlab = "", ylab = "", xaxt = 'n', yaxt = 'n', lwd = 3)
  text(375,-150,paste0("Interation: ", iter))
  points(t(partial_shape_closed_new), type = "l" , col= "red", lwd = 3)
  points(t(partial_shape_closed_mis), type = "l" , col= "blue", lwd = 3)
  
  
  n <- dim(b)[1]
  iter <- 1
  eps <- 15
  
  for (iter in 0:500){
    
    v <- partial_shape_closed_mis - complete_shape_mis
    
    #Computing each basis component and then adding them.  
    gradE <- matrix(0,nrow = 2,ncol = N)
    for (j in 1:n){
      for (k in 1:d){
        val <- trapz(tn,v[k,]*b[j,])*b[j,]
        gradE[k,] <- gradE[k,]+val
      }}
    
    ngE <- trapz(tn,apply(gradE*gradE,2,sum))
    
    partial_shape_closed_mis <- partial_shape_closed_mis-eps*gradE
    
    partial_shape_closed_new <- cbind(partial_shape_closed_obs,partial_shape_closed_mis[,2:dim(partial_shape_closed_mis)[2]])
    
    ngE
    
    
    iter=iter+1
  
    
    if (plot == TRUE & iter%in% c(1,2,3,4,100,200,300,400,500)){
    #plot(t(partial_shape_closed_new), type = "l")
    #points(t(donor), type = "l" , col= "red")
    plot(t(donor), type = "l", xlim = c(-100,550), xlab = "", ylab = "", xaxt = 'n', yaxt = 'n', lwd = 3)
      text(375,-150,paste0("Interation: ",iter))
  points(t(partial_shape_closed_new), type = "l" , col= "red", lwd = 3)
  points(t(partial_shape_closed_mis), type = "l" , col= "blue", lwd = 3)
    }
    
    
  }
  
  
  
  
  #out <- list(partial_shape_imputed = partial_shape_closed_new, partial_obs = partial_shape_closed_obs,  donor = donor)
  #return(out)
  
  


@

\caption{Test}

\end{figure}


Required scripts and data are loaded into the R environment from external files. R package fdasrvf is used for requisite shape distance functions, namely for resampling points around a curve and finding the distance between two closed shapes.

A loop begins for both the individual teeth and their left and right halves. The two halves are generated at this step via finding the two points which are separated with the smallest Euclidean distance, provided that these two points are roughly twenty indices away in either direction, ensuring that teeth will be split roughly at the midpoint, where the X would be on a figure-eight. 

Teeth halves are stored as ``partial shape" alongside an indicator for whether it is the first or second half of the tooth in question, and the complete shape list is amended to not have the tooth in question present so that it would not be matched with itself. 

Using the partial shape distance function, distances between the given half of the given tooth and the remaining completed teeth are taken. From the K smallest distances, a sample of size M with replacement is taken. For each of unique teeth in M, an imputed version of the tooth is then created: First, the partial shape is closed via a series of tightly-spaced points connecting the starting and ending point. Both the partial and complete shape are then resampled with splines—for the partial shape, the straight segment connecting the start and end is done separately from this, in order to avoid spline-based issues where the shape is predicted to curve inwards in a manner not reflective of the true shape. 

The partial and whole shape are then compared in order to find the portion of the complete shape which the observed partial shape most adequately corresponds to. This is done by comparing a series of starting segments of the completed shape to the observed partial shape. For each of the subsets, the partial complete shape is scaled and centered, then a best rotation onto the observed partial shape is made. From this, an error term is found, and the segment with the minimum error term is selected as where the closed partial shape will then be overlaid. From here, the straight line segment from earlier is then gradually morphed so that it more and more adequately fits the curvature of the complete shape. After NUMBER iterations, a final version of the imputed partial tooth is obtained.

Classification is done using KNN, where the distances between the imputed tooth and the whole teeth are measured, and the most common class designation amongst the TEN ? teeth with the smallest distance measure is used to predict the class designation of the imputed tooth. A total of M of these predictions is then obtained based on the sampling from the K smallest distances earlier. Further, KNN classification based off the earlier, partial distances is used a means of assessing the performance of classifying based upon imputation. 

\section{Simulations Study}


\section{Results}


\section{Conclusions}













What if we took all the complete shapes and aligned them 



We can represent shapes using finite representations.\\
An issue here though is that these are really just one realization from an equivalence class.   \\
MAR here means that the proability of missingness is a function of only the observes part of the shape. This seems \citet{deRuiter2008} inrealistic in our setting.  






\bibliographystyle{chicago}
\bibliography{shapebib}


\end{document}